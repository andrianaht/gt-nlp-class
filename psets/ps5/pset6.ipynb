{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 6. Distributed Embeddings and Semi-Supervised Learning #\n",
    "\n",
    "In this assignment, you will use singular value decomposition (SVD) as well as Word2Vec to learn about lexical semantics. You will have to work with \"big\" data\n",
    "- big enough that you will have to think carefully about speed and memory. Of particular importance will **sparse** matrix\n",
    "representations of your data. For this problem you will be submitting pdf version of ipython with outputs along with original ipython. Some particular functions and classes you might need:\n",
    "\n",
    "- [scipy.sparse.csr_matrix](http://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html) - matrix in compressed sparse row format\n",
    "- [scipy.sparse.diags](http://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.diags.html) - method for creating sparse diagonal matrices\n",
    "- [diagonal()](http://docs.scipy.org/doc/numpy/reference/generated/numpy.diagonal.html) - get the diagonal of a matrix\n",
    "- [sklearn.preprocessing.normalize](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.normalize.html) - efficiently normalize sparse matrices\n",
    "- [scipy.sparse.csr_matrix.asfptype()](http://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.asfptype.html) - upcast matrix to a floating point format\n",
    "- [scipy.cluster.vq.kmeans2](http://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.vq.kmeans2.html#scipy.cluster.vq.kmeans2) - Classify a set of observations into k clusters using the k-means algorithm\n",
    "- [numpy.argsort](http://docs.scipy.org/doc/numpy/reference/generated/numpy.argsort.html) - returns the indices that would sort an array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pylab import has clobbered these variables: ['hstack', 'normalize']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "from scipy.sparse.linalg import svds\n",
    "from scipy.sparse import hstack, diags, csr_matrix\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "import csv\n",
    "# from word2vec import VectorModel\n",
    "# from numpy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def csv2csr(filename):\n",
    "    word = []\n",
    "    context = []\n",
    "    count = []\n",
    "    with open(filename,'rb') as infile:\n",
    "        reader = csv.reader(infile)\n",
    "        for row in reader:\n",
    "            word.append(int(row[0]))\n",
    "            context.append(int(row[1]))\n",
    "            count.append(int(row[2]))\n",
    "    return csr_matrix((count,(word,context)))\n",
    "\n",
    "def readVocab(filename):\n",
    "    vocab = []\n",
    "    with open(filename,'rb') as vocabfile:\n",
    "        for line in vocabfile:\n",
    "            vocab.append(line.split()[0])\n",
    "    index = dict(zip(range(0,len(vocab)),vocab)) #from numbers to words\n",
    "    inv_index = {j:i for i,j in index.items()} #from words to numbers\n",
    "    return index,inv_index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the Data ##\n",
    "\n",
    "Call **C=proj4_starter.csv2csr('doc_trips.csv')** to load a sparse matrix $C$ of a word-document counts. The cell $c[i,j]$ should \n",
    "hold the count of word $i$ in document $j$.\n",
    "\n",
    "Call **idx, iidx=proj4_starter.readVocab('vocab.10k')** to load the vocabulary. You get two **dict** objects, mapping between words \n",
    "and indices in the matrix $C$. In **C[iidx['Obama'],:]**, you have the document counts for the word *Obama*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9993, 1000000)\n"
     ]
    }
   ],
   "source": [
    "C = csv2csr('data/ps6/doc_trips.csv')\n",
    "idx, iidx = readVocab('data/ps6/vocab.10k')\n",
    "print C.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cosine Similarity ##\n",
    "\n",
    "The *cosine similarity* of two vectors $u$ and $v$ is defined as \n",
    "$$\\frac{\\sum_{i}u_iv_i}{\\sqrt{\\sum_iu_i^2\\sum_iv_i^2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Deliverable 2a** Consider the words *coffee, play, crazy, facebook*, and *hermana* (Spanish for *sister*). For each of them, find the 10 most similar words according to cosine similarity of the rows in $C$. (2 points)\n",
    "\n",
    "**Hint** The size of the vocabulary is nearly 10,000 words. You do not want to compute and store the entire $10K\\times 10K$ matrix \n",
    "of cosine similarities. Rather, you want to compute them on demand for a given row of the matrix. You may also want to do some\n",
    "precomputation to take care of denominator in advance. Whatever you do, don't lose the sparsity of $C$, or you will not be able \n",
    "to store it.\n",
    "\n",
    "**Sanity check** For *facebook*, the top 5 words I get are *facebook page on twitter deleted instagram*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Here is the word list\n",
    "word_list = ['coffee','play','crazy','facebook','hermana']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def normalizeRow(x):\n",
    "    '''\n",
    "    Normalize each row of x\n",
    "    '''\n",
    "    return diags(np.array(1./(1e-6+np.sqrt(x.multiply(x).sum(axis=1))))[:,0],0) * x\n",
    "\n",
    "def computeCosSimPerWord(word_idx, x):\n",
    "    '''\n",
    "    For a given data matrix, compute cosine similarity between the word with index \"word_index\" and all words \n",
    "    (including itsself)\n",
    "    \n",
    "    Should return a 1-D np.array, not a matrix.\n",
    "    '''\n",
    "    u = x[word_idx]\n",
    "    sim = x.dot(u.transpose())#/x.dot(x.transpose()).multiply(u.dot(u.transpose())).sum()\n",
    "    return np.array([i[0] for i in sim.toarray()])\n",
    "\n",
    "normalizedC = normalizeRow(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "printSimilarWords  is used to print the top 10 similar words to a given word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def printSimilarWords(x, word_list, sim_func, vocab=idx, ivocab=iidx):\n",
    "    for word in word_list:\n",
    "        print word, ':', \n",
    "        word_idx = ivocab[word]\n",
    "        sim_idx = np.argsort(-sim_func(word_idx, x))[:10]\n",
    "        for word2_idx in sim_idx:\n",
    "            print vocab[word2_idx],\n",
    "        print ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coffee : coffee mug shop starbucks drinking drink cup cups and large \n",
      "play : play to games game the and with soccer i . \n",
      "crazy : crazy 's how that it shit is drives i . \n",
      "facebook : facebook page on twitter deleted instagram status compra post whatsapp \n",
      "hermana : hermana mi concha la y de tu con regalo que \n"
     ]
    }
   ],
   "source": [
    "printSimilarWords(normalizedC, word_list, sim_func=computeCosSimPerWord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Deliverable 2b ** Come up with five words of your own that you think might be interesting, and list the top 10 most similar for each. Try to choose a few different types of words, such as verbs, adjectives, names, emotions, abbreviations, or alternative spellings. (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "additional_word_list = ['eat', 'however', 'bad', 'obama', 'lmao']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eat : eat food i to and hungry healthy meat cake sleep \n",
      "however : however spell holidays prob vision , . unfortunately the opinion \n",
      "bad : bad breaking so feel i a it mood . that \n",
      "obama : obama president michelle classy pres 2012 says america actions clip \n",
      "lmao : lmao @u i that was you my the like it \n"
     ]
    }
   ],
   "source": [
    "printSimilarWords(normalizedC, additional_word_list, sim_func=computeCosSimPerWord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Document Co-occurence ##\n",
    "\n",
    "Compute the document co-occurence matrix $D$, where $d_{i,j}$ is the probability $P(w_j|w_i)$ that word $j$ appears in a tweet, \n",
    "given that word $i$ appears. To do this, first compute the co-occurence counts $CC^\\top$. Substract the diagonal, then normalize \n",
    "each row. \n",
    "\n",
    "Note: it is possible to smooth this probability, but if you naively add some number to the matrix, you will lose sparsity \n",
    "and memory will blow up. You can do it unsmoothed. However, smoothing is not required here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Deliverable 3** For each of the 10 examples above (my five words and your five words), find the 10 most similar words according to cosine similarity of the rows of $D$. (2 points)\n",
    "\n",
    "**Sanity check** For *facebook*, the 5 words I get are *facebook instagram twitter tv youtube*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9993, 9993)\n"
     ]
    }
   ],
   "source": [
    "def computeCooccurMatrix(C):\n",
    "    '''\n",
    "    Compute the co-occurence matrix D\n",
    "    '''\n",
    "    D = C.dot(C.transpose())\n",
    "    D.setdiag(0)\n",
    "    return D\n",
    "\n",
    "D = computeCooccurMatrix(C)\n",
    "normalizedD = normalizeRow(D)\n",
    "print normalizedD.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_list = word_list + additional_word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coffee : coffee new today drinking getting food after day eating water \n",
      "play : play down be back pass show beat them up run \n",
      "crazy : crazy weird sad not stupid damn actually cool about over \n",
      "facebook : facebook instagram twitter tv youtube tumblr 100 note ex insta \n",
      "hermana : hermana abuela hermano vieja novio novia viejo padre familia corazon \n",
      "eat : eat get drink have cry wear really just watch leave \n",
      "however : however although especially also &amp; after as quite perfect under \n",
      "bad : bad damn actually just like sad i'm but weird fucking \n",
      "obama : obama nick coach poor talent mike action john tom ryan \n",
      "lmao : lmao lmfao lol bruh yea yeah omg aha lmaoo lolol \n"
     ]
    }
   ],
   "source": [
    "printSimilarWords(normalizedD, word_list, sim_func=computeCosSimPerWord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Latent Semantic Analysis ##\n",
    "\n",
    "Perform truncated SVD (**scipy.sparse.linalg.svds**) to obtain $USV^\\top\\approx C$ using $K=10$. Each row vector $u_i$\n",
    "is a description of the word $i$. You can compute similarity between pairs of words using the squared Euclidean norm \n",
    "$\\|u_i-u_j\\|^2_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Deliverable 4(a)** For each of the 10 examples above, find the 10 most similar words according to squared Euclidean distance in $U$. (3 points)\n",
    "\n",
    "**Sanity check** For *facebook*, the top 5 words are *facebook ex harry calls snap *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def computeEuclidDist(word_idx, U,with_S = False):\n",
    "    '''\n",
    "    Compute the Euclid distance bwteen word with index \"word_idx\" and all words \n",
    "    (including itself)\n",
    "    \n",
    "    Args:\n",
    "      word_idx - the word index\n",
    "      U - latent representation of words\n",
    "    Return:\n",
    "        Euclidean distance from representation of word_idx to all words\n",
    "    '''\n",
    "#     u = U[0]\n",
    "    ui = U[word_idx]\n",
    "    dis = np.sum(np.power(U - ui, 2), 1)\n",
    "    return dis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Once you finish the function computeEuclidDist, run the following code directly to print results\n",
    "'''\n",
    "Cfp= C.asfptype()\n",
    "U, _, _ = svds(Cfp, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coffee : coffee drinking paper cat eating apparently turned short dress months \n",
      "play : play looking everyone anyone another start coming use enough away \n",
      "crazy : crazy gone stupid sad cool yet funny perfect must once \n",
      "facebook : facebook ex harry calls snap instagram ...... uh 18 fav \n",
      "hermana : hermana unas minutos colegio toca novia verte gana juego unos \n",
      "eat : eat wake wanted use idk own hair wear bc might \n",
      "however : however unfortunately dry repeat hip seven sweat jackson slightly fed \n",
      "bad : bad ass long being its than every little while fucking \n",
      "obama : obama 1/2 70 #1 moyes dc education leads murder ghana \n",
      "lmao : lmao omg tho remember until wouldn't heard thinking cant wonder \n"
     ]
    }
   ],
   "source": [
    "printSimilarWords(U, word_list, sim_func=lambda word_idx, U : -computeEuclidDist(word_idx,U))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Deliverable 4(b) ** Now compute the same SVD with $K=50$, and again find the 10 most similar words according to Euclidean distance $U$. (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coffee : coffee tea wine four gas midnight starbucks service wedding six \n",
      "play : play hang run chill we're agree nobody kids fight games \n",
      "crazy : crazy weird stupid actually af dumb serious dead ugly dude \n",
      "facebook : facebook instagram focus ig netflix earth floor fb tl list \n",
      "hermana : hermana sonrisa llevo plata jajajaj realidad arriba jajaj vuelta novia \n",
      "eat : eat drink cry wear smoke food lazy buy drunk figure \n",
      "however : however criminal despite peeps caps included hidden honesty mass understanding \n",
      "bad : bad far hard funny cute sad cool long true damn \n",
      "obama : obama rooney kevin suarez bjp johnson sterling russia injured peter \n",
      "lmao : lmao dude tho smh lmfao yea af bruh ago kid \n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "U50, _, _ = svds(Cfp, 50)\n",
    "printSimilarWords(U50, word_list, sim_func=lambda word_idx, U : -computeEuclidDist(word_idx,U))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Deliverable 4(c) ** Now compute the SVD of the matrix $\\mathbf{D}$, using with $K = 10$, and $K = 50$. Report \n",
    "the most similar words to each of the example words according to Euclidean distance in $U$. (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coffee : coffee upset leaving crap bullshit boring nights loud losing reading \n",
      "play : play anyone use looking move forward stop win went music \n",
      "crazy : crazy gone mind forever kind hot full fun sick fucked \n",
      "facebook : facebook 24 david alcohol simple 17 traffic 22 priority med \n",
      "hermana : hermana entrar esperar juego colegio saben unico culpa minutos unas \n",
      "eat : eat leave hear live wake stay early ask wear sometimes \n",
      "however : however jackson hill 5th spain george jordan sterling papers cars \n",
      "bad : bad than fucking look ass doing getting already actually made \n",
      "obama : obama available dallas mobile lessons russia ireland services 1/2 tom \n",
      "lmao : lmao omg tweet guys tho ?! xx bro dude funny \n"
     ]
    }
   ],
   "source": [
    "# your code here for K = 10\n",
    "Dfp= D.asfptype()\n",
    "Ud, _, _ = svds(Dfp, 10)\n",
    "printSimilarWords(Ud, word_list, sim_func=lambda word_idx, U : -computeEuclidDist(word_idx,U))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coffee : coffee cream tea ice cheese starbucks chicken chips cake pack \n",
      "play : play run drive beach beat party walk ball join stay \n",
      "crazy : crazy af weird dumb dying freaking gay crying confused stupid \n",
      "facebook : facebook instagram ig netflix focus couch nerves insta floor fb \n",
      "hermana : hermana vieja verga abuela peli loca ropa tarea prueba siesta \n",
      "eat : eat wear drink asleep smoke cry finish break decided sit \n",
      "however : however muslim 28 although 26 among % therefore nor previous \n",
      "bad : bad far cute sad long funny damn weird annoying rn \n",
      "obama : obama goalie rape nt manziel regular false durant melo emoji \n",
      "lmao : lmao dude tho lmfao wow yea bro bet bruh forgot \n"
     ]
    }
   ],
   "source": [
    "# your code here for K = 50\n",
    "Ud50,_, _ = svds(Dfp, 50)\n",
    "printSimilarWords(Ud50, word_list, sim_func=lambda word_idx, U : -computeEuclidDist(word_idx,U))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coffee : coffee tea wine cheese chicken glass workout sandwich milk truck \n",
      "play : play players played against ball agree lebron team player beat \n",
      "crazy : crazy weird fast boring fake bullshit stupid ugly lame dumb \n",
      "facebook : facebook instagram netflix focus ig floor earth couch list nerves \n",
      "hermana : hermana llevar peli abuela plata vieja chica novia hija verga \n",
      "eat : eat drink wear shopping skip ahead cook finish smoke classes \n",
      "however : however therefore standards williams iran despite speaker robert activity receive \n",
      "bad : bad cute far annoying weird pretty hot stupid damn rn \n",
      "obama : obama melo nick injured sterling suarez kevin jones peter bosh \n",
      "lmao : lmao dude lmfao bruh tho smh yea wow af bro \n"
     ]
    }
   ],
   "source": [
    "# optionally, try K = 100 and see if it's even better\n",
    "Ud100,_, _ = svds(Dfp, 100)\n",
    "printSimilarWords(Ud100, word_list, sim_func=lambda word_idx, U : -computeEuclidDist(word_idx,U))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Local Context ##\n",
    "\n",
    "Local context captures the frequency with which words appear in each others’ immediate context. We have provided a CSV file (succ_trips_50k.csv)  in which each line contains a triple $\\langle x,y,z\\rangle$, \n",
    "where $x$ and $y$ are term IDs and $z$ is the count of times where $y$ immediately follows $x$ . \n",
    "The vocabulary has now increased to 50K words. There is an associated vocabulary file, **vocab.50k**.\n",
    "\n",
    "**Deliverable 5a **\n",
    "Build a sparse matrix $\\mathbf{E}$ from these triples. Normalize the rows of $\\mathbf{E}$, such that $e_{i,j}=\\frac{n(i,j)}{n(i)}$, the probability of seeing word $j$ given that you have just seen word $i$. \n",
    "Now form a matrix $\\mathbf{F} = [\\mathbf{E}~ \\mathbf{E}’]$ by horizontally concatenating the normalized matrix $\\mathbf{E}$. You will perform sparse singular value decomposition on $\\mathbf{F}$. (2 points)\n",
    "\n",
    "**Hint** make sure you are using a sparsity-preserving operation to combine E and E'!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idx_50, iidx_50 = readVocab('data/ps6/vocab.50k')\n",
    "E = csv2csr('data/ps6/succ_trips_50k.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "def constructF(E):\n",
    "    '''\n",
    "    Finish the following code to construct F from E\n",
    "    '''   \n",
    "    Enorm = normalizeRow(E)\n",
    "    return hstack([Enorm, Enorm.T])\n",
    "    \n",
    "F = constructF(E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Deliverable 5b ** For $K = 10$ and $K = 50$ compute the top 10 synonyms for each of your ten words. (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coffee : coffee gold victory purple women learning track line speech story \n",
      "play : play start playing eat live through watch top doing win \n",
      "crazy : crazy monday sad card fun times session plant slow died \n",
      "facebook : facebook cnn humble fest pots andre gracious giro mature 42 \n",
      "hermana : hermana hermano devant avc vite pilas grosse hubo estudie emos \n",
      "eat : eat live pay driving sell west leave play chicken walk \n",
      "however : however sincerely ect slc luhan cech suram bjss giliran kecewa \n",
      "bad : bad job different long fan fat bitch movie goal real \n",
      "obama : obama relax epic 32 summit gd subtweets tom ukip wednesday \n",
      "lmao : lmao lmfao omg hahahah hahahahahaha hahahahah amen shhh imy kmsl \n"
     ]
    }
   ],
   "source": [
    "# K = 10\n",
    "U_f,S_f,V_f = svds(F, 10)\n",
    "# note that we have to insert the new vocabulary as an optional argument\n",
    "printSimilarWords(U_f, word_list, vocab=idx_50,ivocab=iidx_50, sim_func=lambda word_idx, U : -computeEuclidDist(word_idx,U))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coffee : coffee milk selfies management bacon citizen ministers jelly folding hawaiian \n",
      "play : play fly meet committed drive prove speaks stick learn decide \n",
      "crazy : crazy dead scary funny dope intense annoying awful disgusting upsetting \n",
      "facebook : facebook netflix youtube saturdays pinterest i-45 focused tabs sundays twitter \n",
      "hermana : hermana piace costaba avete siendo repita sento menea hepimiz prennent \n",
      "eat : eat try earn meet declare peel convert receive pretend destroy \n",
      "however : however citrus blvd terk 0kph dormirei eaai bjbj smoothly sincerely \n",
      "bad : bad long fake special bitch fat girl single grown fuxking \n",
      "obama : obama manziel convenient kejriwal rigged surgeon mandatory #22jumpstreet jojo pattern \n",
      "lmao : lmao lmfao omg yeah lmaooo ctfu yea lmfaoo omfg lolz \n"
     ]
    }
   ],
   "source": [
    "# your code for K = 50 here\n",
    "U50_f,S50_f,V50_f = svds(F, 50)\n",
    "printSimilarWords(U50_f, word_list, vocab=idx_50, ivocab=iidx_50, sim_func=lambda word_idx, U : -computeEuclidDist(word_idx,U))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deliverable 5c ** Overall, which set of synonyms looks best to you? \n",
    "Count how many of the top 5 synonyms for *coffee* and   *crazy*\n",
    "have the same majority part of speech (e.g., *play* is a verb) as the cue word.\n",
    "Use the tagset from the [Twitter POS paper](http://www.cc.gatech.edu/~jeisenst/papers/acl2012pos.pdf). Does local context or document context do better at matching the POS of the cue words? Why? (Consider first context provided by wordnet as majority POS). (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(Your answer here)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Word2Vec ##\n",
    "As mentioned in the class Word2Vec are distributed continuous vector representations  for words.\n",
    "In this part, you will be building and training Word2Vec module keeping in mind various hyper parameters such as min_count, window_size etc.(10 points)\n",
    "You will be using Gensim http://radimrehurek.com/gensim/ to train your model.\n",
    "\n",
    "- For instructions on how to install gensim, see [here](https://radimrehurek.com/gensim/install.html). \n",
    "- I strongly recommend that you test your install by downloading the [source](http://pypi.python.org/pypi/gensim) and running ```python setup.py test```\n",
    "- For a tutorial on how to use gensim for word embeddings, see [here](http://rare-technologies.com/word2vec-tutorial/)\n",
    "- It is recommended that you have a working c compiler, so that the much faster cythonized gensim can run. This is transparent to you, but does require that you have a c compiler installed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "        print \"Opening the file...\"\n",
    "\n",
    "        X_train = []\n",
    "\n",
    "        f = open(filename,'r')\n",
    "        count = 0\n",
    "        for line in f.readlines():\n",
    "            sentence = []\n",
    "            line = line.strip()\n",
    "            if not line: continue\n",
    "            try:\n",
    "                sentence = word_tokenize(line)\n",
    "            except:\n",
    "                pass\n",
    "            if(len(sentence) > 2):\n",
    "                count =count+1\n",
    "                X_train.append(array(sentence))\n",
    "            # else:\n",
    "            #      print \"No words\"\n",
    "            #      print sentence\n",
    "\n",
    "        print \"File successfully read\"\n",
    "        print count , \"sentences\"\n",
    "        f.close()\n",
    "        return array(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening the file...\n",
      "File successfully read\n",
      "17392 sentences\n"
     ]
    }
   ],
   "source": [
    "sentences = read_data('data/ps6/wiki.train.en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build and train model you will be using [Gensim](http://radimrehurek.com/gensim/). \n",
    "\n",
    "This [tutorial](https://radimrehurek.com/gensim/models/word2vec.html) may help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deliverable 6a**: Build a model with embedding size = 10, and build a vocabulary from the data, with min_count = 10. Print the vocabulary size, which should be between 9000 and 10000. (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.word2vec:consider setting layer size to a multiple of 4 for greater performance\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size 9911\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "model = Word2Vec(size=10, min_count=10)\n",
    "model.build_vocab(sentences)\n",
    "print 'Vocab Size', len(model.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deliverable 6b**: Train your model for one iteration, and report the similarity between the words (\"cat\" and \"dog\"), (\"night\" and \"day\"), and (\"can\" and \"fish\"). (2 points)\n",
    "\n",
    "**Hint**: before training on the whole dataset, train on sentences[0:5] to make sure it works and isn't too slow. On my laptop, one iteration takes 2-3 seconds. \n",
    "\n",
    "**Sanity check** the similarity I get between \"cat\" and \"dog\" is 0.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat dog -0.185590503808\n",
      "night day 0.46156263486\n",
      "can fish 0.0214166420165\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "model.train(sentences)\n",
    "for tup in [ (\"cat\" , \"dog\"), (\"night\" , \"day\"),  (\"can\" , \"fish\") ]:\n",
    "    print tup[0], tup[1], model.similarity(tup[0] , tup[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.word2vec:consider setting layer size to a multiple of 4 for greater performance\n",
      "WARNING:gensim.models.word2vec:consider setting layer size to a multiple of 4 for greater performance\n",
      "WARNING:gensim.models.word2vec:consider setting layer size to a multiple of 4 for greater performance\n",
      "WARNING:gensim.models.word2vec:consider setting layer size to a multiple of 4 for greater performance\n",
      "WARNING:gensim.models.word2vec:consider setting layer size to a multiple of 4 for greater performance\n",
      "WARNING:gensim.models.word2vec:consider setting layer size to a multiple of 4 for greater performance\n",
      "WARNING:gensim.models.word2vec:consider setting layer size to a multiple of 4 for greater performance\n",
      "WARNING:gensim.models.word2vec:consider setting layer size to a multiple of 4 for greater performance\n",
      "WARNING:gensim.models.word2vec:consider setting layer size to a multiple of 4 for greater performance\n",
      "WARNING:gensim.models.word2vec:consider setting layer size to a multiple of 4 for greater performance\n",
      "WARNING:gensim.models.word2vec:consider setting layer size to a multiple of 4 for greater performance\n",
      "WARNING:gensim.models.word2vec:consider setting layer size to a multiple of 4 for greater performance\n",
      "WARNING:gensim.models.word2vec:consider setting layer size to a multiple of 4 for greater performance\n",
      "WARNING:gensim.models.word2vec:consider setting layer size to a multiple of 4 for greater performance\n",
      "WARNING:gensim.models.word2vec:consider setting layer size to a multiple of 4 for greater performance\n",
      "WARNING:gensim.models.word2vec:consider setting layer size to a multiple of 4 for greater performance\n",
      "WARNING:gensim.models.word2vec:consider setting layer size to a multiple of 4 for greater performance\n",
      "WARNING:gensim.models.word2vec:consider setting layer size to a multiple of 4 for greater performance\n",
      "WARNING:gensim.models.word2vec:consider setting layer size to a multiple of 4 for greater performance\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, -0.18559050380843495)\n",
      "(2, -0.23568871353679591)\n",
      "(3, -0.22201573673069694)\n",
      "(4, -0.14041723026758413)\n",
      "(5, -0.056319806929990418)\n",
      "(6, 0.054927368041018443)\n",
      "(7, 0.16042375484429922)\n",
      "(8, 0.23634676631017865)\n",
      "(9, 0.30109205581079501)\n",
      "(10, 0.35358735886400605)\n",
      "(11, 0.40393237603474819)\n",
      "(12, 0.44006670435641032)\n",
      "(13, 0.4705096827222216)\n",
      "(14, 0.50169483485425803)\n",
      "(15, 0.51993819381285111)\n",
      "(16, 0.53122100154660479)\n",
      "(17, 0.53966607709433301)\n",
      "(18, 0.54765018484556804)\n",
      "(19, 0.55813747202596131)\n"
     ]
    }
   ],
   "source": [
    "res = []\n",
    "for i in range(1, 20):\n",
    "    model = Word2Vec(size=10, min_count=10, iter=i)\n",
    "    model.build_vocab(sentences)\n",
    "    model.train(sentences)\n",
    "    res.append( (i, model.similarity(\"cat\", \"dog\")) )\n",
    "\n",
    "for val in res:\n",
    "    print val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deliverable 6c** Modify the code below to compare the performance of different embedding sizes. (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainMod(model,sentences,max_its=10):\n",
    "    # your code to build the vocabulary\n",
    "    model.build_vocab(sentences)\n",
    "    scores = []\n",
    "    times = []\n",
    "    for _ in xrange(max_its):\n",
    "        # your code to train the model\n",
    "        model.train(sentences)\n",
    "        scores.append(sum(model.score(sentences)))\n",
    "        times.append(model.total_train_time)\n",
    "    return scores,times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.word2vec:terminating after 1000000 sentences (set higher total_sentences if you want more).\n",
      "WARNING:gensim.models.word2vec:terminating after 1000000 sentences (set higher total_sentences if you want more).\n",
      "WARNING:gensim.models.word2vec:terminating after 1000000 sentences (set higher total_sentences if you want more).\n",
      "WARNING:gensim.models.word2vec:terminating after 1000000 sentences (set higher total_sentences if you want more).\n",
      "WARNING:gensim.models.word2vec:terminating after 1000000 sentences (set higher total_sentences if you want more).\n",
      "WARNING:gensim.models.word2vec:terminating after 1000000 sentences (set higher total_sentences if you want more).\n",
      "WARNING:gensim.models.word2vec:terminating after 1000000 sentences (set higher total_sentences if you want more).\n",
      "WARNING:gensim.models.word2vec:terminating after 1000000 sentences (set higher total_sentences if you want more).\n",
      "WARNING:gensim.models.word2vec:terminating after 1000000 sentences (set higher total_sentences if you want more).\n",
      "WARNING:gensim.models.word2vec:terminating after 1000000 sentences (set higher total_sentences if you want more).\n",
      "WARNING:gensim.models.word2vec:terminating after 1000000 sentences (set higher total_sentences if you want more).\n",
      "WARNING:gensim.models.word2vec:terminating after 1000000 sentences (set higher total_sentences if you want more).\n",
      "WARNING:gensim.models.word2vec:terminating after 1000000 sentences (set higher total_sentences if you want more).\n",
      "WARNING:gensim.models.word2vec:terminating after 1000000 sentences (set higher total_sentences if you want more).\n",
      "WARNING:gensim.models.word2vec:terminating after 1000000 sentences (set higher total_sentences if you want more).\n",
      "WARNING:gensim.models.word2vec:terminating after 1000000 sentences (set higher total_sentences if you want more).\n",
      "WARNING:gensim.models.word2vec:terminating after 1000000 sentences (set higher total_sentences if you want more).\n",
      "WARNING:gensim.models.word2vec:terminating after 1000000 sentences (set higher total_sentences if you want more).\n",
      "WARNING:gensim.models.word2vec:terminating after 1000000 sentences (set higher total_sentences if you want more).\n",
      "WARNING:gensim.models.word2vec:terminating after 1000000 sentences (set higher total_sentences if you want more).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with 20\n",
      "done with"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.word2vec:terminating after 1000000 sentences (set higher total_sentences if you want more).\n",
      "WARNING:gensim.models.word2vec:terminating after 1000000 sentences (set higher total_sentences if you want more).\n",
      "WARNING:gensim.models.word2vec:terminating after 1000000 sentences (set higher total_sentences if you want more).\n",
      "WARNING:gensim.models.word2vec:terminating after 1000000 sentences (set higher total_sentences if you want more).\n",
      "WARNING:gensim.models.word2vec:terminating after 1000000 sentences (set higher total_sentences if you want more).\n",
      "WARNING:gensim.models.word2vec:terminating after 1000000 sentences (set higher total_sentences if you want more).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 40\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-30e83247892a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#your code to create a model here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtimes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainMod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'o-'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model-%d'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-bb51962214e4>\u001b[0m in \u001b[0;36mtrainMod\u001b[0;34m(model, sentences, max_its)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_its\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;31m# your code to train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mtimes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_train_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/gensim/models/word2vec.pyc\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, total_words, word_count, chunksize, total_examples, queue_factor, report_delay)\u001b[0m\n\u001b[1;32m    755\u001b[0m                 \u001b[0mjob_no\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjobs_source\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"putting job #%i in the queue at alpha %.05f\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_no\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_alpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m                 \u001b[0mjob_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_alpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m                 \u001b[0;31m# update the learning rate before every next job\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_alpha\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mnext_alpha\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/2.7.9/Frameworks/Python.framework/Versions/2.7/lib/python2.7/Queue.pyc\u001b[0m in \u001b[0;36mput\u001b[0;34m(self, item, block, timeout)\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m                     \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxsize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/2.7.9/Frameworks/Python.framework/Versions/2.7/lib/python2.7/threading.pyc\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0m__debug__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_note\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s.wait(): got it\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZYAAAEGCAYAAABGnrPVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKeVR/w/sY3N49od6IQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x108d8eed0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_k = [20,40,100] # embedding sizes\n",
    "models = []\n",
    "for k in all_k:\n",
    "    model = Word2Vec(size=k, min_count=10) #your code to create a model here\n",
    "    models.append(model)\n",
    "    scores,times = trainMod(model,sentences)\n",
    "    plt.plot(times,scores,'o-');\n",
    "    model.save('model-%d'%(k))\n",
    "    print 'done with',k\n",
    "plt.legend(['k=%d'%(k) for k in all_k],loc='lower right');\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('log likelihood')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deliverable 6d** Use the ```most_similar``` function to find similar words to the items in your word_list which are in the vocabulary for your word2vec models. Compare these most similar words with the outputs of the methods tried earlier in the assignment. You may train the word2vec models for longer if you wish, or change any of the parameters. (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.word2vec:consider setting layer size to a multiple of 4 for greater performance\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coffee [('including', 0.8265149593353271), ('endemic', 0.8208082914352417), ('corn', 0.8206558227539062), ('fruits', 0.815038800239563), ('mining', 0.7832599878311157), ('herbaceous', 0.7749211192131042), ('affiliation', 0.7734283804893494), ('vegetables', 0.7648007869720459), ('minorities', 0.7501692771911621), ('genera', 0.7495756149291992)]\n",
      "\n",
      "play [('Gershwin', 0.9440773129463196), ('manuscript', 0.9060929417610168), ('revised', 0.8952364325523376), ('Ward', 0.8879287838935852), ('seminal', 0.8859313130378723), ('traveler', 0.8811007738113403), ('Bates', 0.880907416343689), ('publishing', 0.871012806892395), ('instrumental', 0.864136815071106), ('publication', 0.8640176057815552)]\n",
      "\n",
      "eat [('appear', 0.8824892044067383), ('prey', 0.870610237121582), ('counted', 0.8687939047813416), ('differentiated', 0.8647506237030029), ('far', 0.8596727848052979), ('belong', 0.8591430187225342), ('predators', 0.8566170334815979), ('longer', 0.8563065528869629), ('fur', 0.8503599762916565), ('dig', 0.8499026894569397)]\n",
      "\n",
      "however [('siblings', 0.9194761514663696), ('albeit', 0.9142251014709473), ('intact', 0.8926364183425903), ('though', 0.887130856513977), ('arrows', 0.8561215996742249), ('older', 0.8491328358650208), ('although', 0.8468226194381714), ('laying', 0.8458315134048462), ('dogs', 0.8453158140182495), ('teachers', 0.8395844101905823)]\n",
      "\n",
      "bad [('decides', 0.9283193349838257), ('breath', 0.9173914790153503), ('indeed', 0.9170480966567993), ('kill', 0.9159603714942932), ('nothing', 0.9105085730552673), ('luck', 0.9079241752624512), ('oneself', 0.8922002911567688), ('clear', 0.884787917137146), ('thinks', 0.8827095031738281), ('goes', 0.8781067728996277)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "model = Word2Vec(size=10, min_count=10, iter=20)\n",
    "model.build_vocab(sentences)\n",
    "model.train(sentences)\n",
    "\n",
    "for word in word_list:\n",
    "    if model.vocab.get(word, None) is not None:\n",
    "        print word, model.most_similar(word, topn=10)\n",
    "        print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.word2vec:consider setting layer size to a multiple of 4 for greater performance\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coffee [('pine', 0.991111159324646), ('comprising', 0.9804106950759888), ('mountain', 0.9742515087127686), ('wool', 0.9692439436912537), ('sediments', 0.9602529406547546), ('latitudes', 0.951688826084137), ('subtropical', 0.9508953094482422), ('Alpine', 0.950741171836853), ('limestone', 0.9493807554244995), ('vehicle', 0.9488292932510376)]\n",
      "\n",
      "play [('colleague', 0.9956479072570801), ('replied', 0.992526650428772), ('longtime', 0.9918588399887085), ('Orwell', 0.9910629987716675), ('Hastings', 0.9883396625518799), ('told', 0.9870568513870239), ('Donald', 0.9862906336784363), ('Ginsberg', 0.982170581817627), ('paper', 0.9816673398017883), ('stated', 0.980459988117218)]\n",
      "\n",
      "eat [('pain', 0.9968546628952026), ('contact', 0.9915989637374878), ('apple', 0.9912633299827576), ('long', 0.9886532425880432), ('although', 0.9867517352104187), ('frame', 0.985264241695404), ('convert', 0.9847733974456787), ('shown', 0.9832981824874878), ('treat', 0.9795106649398804), ('wires', 0.9776336550712585)]\n",
      "\n",
      "however [('right', 0.9975627064704895), ('stopping', 0.9945693016052246), ('accomplished', 0.9915959239006042), ('seem', 0.9872153401374817), ('sight', 0.9867086410522461), ('speak', 0.984646201133728), ('serious', 0.9843796491622925), ('judged', 0.9843689203262329), ('satisfy', 0.984012246131897), ('little', 0.9829058051109314)]\n",
      "\n",
      "bad [('prove', 0.9973235130310059), ('nothing', 0.9965361952781677), ('tell', 0.9950292706489563), ('say', 0.9941743016242981), ('happy', 0.9921863675117493), ('flesh', 0.992179274559021), ('hide', 0.991850733757019), ('something', 0.9912768602371216), ('knowing', 0.9891757965087891), ('know', 0.9883795976638794)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec(size=5, min_count=10, iter=20)\n",
    "model.build_vocab(sentences)\n",
    "model.train(sentences)\n",
    "\n",
    "for word in word_list:\n",
    "    if model.vocab.get(word, None) is not None:\n",
    "        print word, model.most_similar(word, topn=10)\n",
    "        print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(your explanation here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the claims about word2vec word embeddings is that they can solve analogy problems, like \"man:woman::king:queen\"\n",
    "\n",
    "[This tutorial](http://rare-technologies.com/word2vec-tutorial/) contains example code showing how to use word vectors to try to solve analogies.\n",
    "\n",
    "**Deliverable 6e** Using the three models trained in the previous deliverable, see if each can solve the analogies man:woman::king:queen and architect:building::painter:painting. Invent two more analogies, and see which of the three models can solve them. You can Google analogies, but you may have to search for a while to find analogies where all four elements are in the vocabulary. You can also retrain the models with a larger vocabulary if you want. (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening the file...\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "filename = 'oanc'\n",
    "from nltk.tokenize import sent_tokenize\n",
    "print \"Opening the file...\"\n",
    "f = open(filename,'r')\n",
    "count = 0\n",
    "for line in f.readlines():\n",
    "    line = line.decode('utf-8').strip()\n",
    "    corpus = [ sent for sent in sent_tokenize(line) ]\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13655748"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grep '[^[:blank:]]' < out.txt |   tr -cd \"[:print:]\" | tr \"\\t\" \" \" | tr -s \" \" \" \"  | tr -cd '[:print:]' > oanc\n",
    "\n",
    "import os\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "class MySentences(): \n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "    \n",
    "    def __iter__(self):\n",
    "        f = open(filename,'r')\n",
    "        for line in f.readlines():\n",
    "            line = line.decode('utf-8')\n",
    "            for sent in sent_tokenize(line):\n",
    "                yield word_tokenize(sent)\n",
    "                \n",
    "class MySentences(): \n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    "                \n",
    "    def __iter__(self):\n",
    "        for base, dirs, files in os.walk(self.dirname):\n",
    "            for fname in files:\n",
    "                if fname.endswith(\".txt\"):\n",
    "                    for line in open(os.path.join(base, fname)):\n",
    "                        yield line.split()        \n",
    "                        \n",
    "dirname = 'data/oanc'\n",
    "sentences = MySentences(dirname)\n",
    "model = Word2Vec(size=200, window=5, min_count=10, workers=6)\n",
    "model.build_vocab(sentences)\n",
    "model.train(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('mother', 0.6708232164382935)]\n",
      "[('aunt', 0.6285437345504761)]\n",
      "[('Italy', 0.6088502407073975)]\n",
      "[('anxiety', 0.4334152042865753)]\n"
     ]
    }
   ],
   "source": [
    "print model.most_similar(positive=['woman','father'],negative=['man'], topn=1) # woman:mother::man:father\n",
    "print model.most_similar(positive=['brother','sister'],negative=['father'], topn=1) # boy:son::girl:daughter\n",
    "print model.most_similar(positive=['France', 'Germany'],negative=['Paris'], topn=1) # Spain:Faro::France:Paris\n",
    "print model.most_similar(positive=['bad', 'worse'],negative=['big'], topn=1) # bad worse big bigger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Semi-supervised learning #\n",
    "\n",
    "Now you will use the word embeddings to try to improve your dependency parser from problem set 5.\n",
    "\n",
    "You will do this by clustering words according to their embedding.\n",
    "\n",
    "The code below run the clustering algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.vq import kmeans2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11041330"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Word2Vec(size=40, min_count=10, iter=10)\n",
    "model.build_vocab(sentences)\n",
    "model.train(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# choose a model to use here\n",
    "word_vectors = model.syn0\n",
    "num_clusters = 150\n",
    "# run the clustering algorithm\n",
    "centroids, labels = kmeans2(word_vectors,num_clusters,iter=100,minit='points')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deliverable 7a** Modify the code below to show the words in the same cluster as a query word. Find the words in the same cluster as \"coffee\", \"computer\", and \"red\" for at least one of your models, plus any other words of interest. (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['species', 'plants', 'contain', 'crops', 'algae', 'grown', 'almond', 'genera', 'almonds', 'trees']\n",
      "['use', 'using', 'data', 'information', 'program', 'computer', 'design', 'sound', 'Ada', 'machine']\n",
      "['air', 'surface', 'base', 'white', 'black', 'inside', 'covered', 'red', 'walls', 'color']\n"
     ]
    }
   ],
   "source": [
    "def getWordsInSameCluster(word,model,labels):\n",
    "    # you will use model.index2word to make this function\n",
    "    # return a list of words in the same cluster as word\n",
    "    if word in model.index2word:\n",
    "        label = labels[model.index2word.index(word)]\n",
    "        idx = [ i for i, lbl in enumerate(labels) if label == lbl ]\n",
    "        return [ model.index2word[i] for i in idx ]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "print getWordsInSameCluster('coffee', model, labels)[:10]\n",
    "print getWordsInSameCluster('computer', model, labels)[:10]\n",
    "print getWordsInSameCluster('red', model, labels)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving dependency parsing ##\n",
    "\n",
    "Now incorporate these cluster features into your best performing parser from project 5, based on the dev data. Add a feature for each cluster/tag pair, e.g., C175/N, C189/V, etc. You will then compute the accuracy with training sets of various sizes, comparing the performance of your model with and without the cluster features.\n",
    "\n",
    "**Deliverable 7b ** Build training sets including the first 50, 100, 200, 500, and 1000 *sentences* (not words). \n",
    "Train your tagger on each training set, using your original features, and plot the accuracy on the development set. \n",
    "Then retrain you tagger, including the new word cluster features, and plot accuracy on the development set on the same plot. \n",
    "Run for at least 10 iterations in each case.\n",
    "\n",
    "You may want to try larger numbers of clusters to improve performance. \n",
    "\n",
    "You can even include the results from multiple clusterings with different numbers of clusters (50, 100, 200, 500, ...), \n",
    "using features like C43-50/N (cluster 43 of 50, with tag N), C377-500/V (cluster 377 of 500, with tag V).\n",
    "\n",
    "You can also use clusters for the features of neighboring words.  (6 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from os.path import join\n",
    "import gtparsing\n",
    "import gtparsing.dependency_parser as depp\n",
    "import gtparsing.dependency_features as depf\n",
    "import gtparsing.custom_features\n",
    "import gtparsing.utilities\n",
    "\n",
    "from score import accuracy\n",
    "from gtparsing.custom_features import BakeoffFeats\n",
    "\n",
    "class DependencyReader():\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "class W2V_ClusterFeats(BakeoffFeats):\n",
    "    def __init__():\n",
    "        pass\n",
    "    \n",
    "    def create_arc_features(self,instance,h,m,add=False):\n",
    "        ff = super(W2V_ClusterFeats, self).create_arc_features(instance,h,m,add)\n",
    "        k = len(ff)\n",
    "        return ff\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 100\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "read_data() takes exactly 2 arguments (3 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-a056cfdf2169>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m'Train 100'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdepp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDependencyParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBakeoffFeats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtr_acc\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mdv_acc\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mdp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_perceptron\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: read_data() takes exactly 2 arguments (3 given)"
     ]
    }
   ],
   "source": [
    "print 'Train 100'\n",
    "dp = depp.DependencyParser(feature_function=BakeoffFeats())\n",
    "dp.read_data(\"english\",100)\n",
    "tr_acc ,dv_acc  = dp.train_perceptron(10) \n",
    "\n",
    "print 'Train 100'\n",
    "dp = depp.DependencyParser(feature_function=W2V_ClusterFeats())\n",
    "dp.read_data(\"english\",100)\n",
    "tr_acc ,dv_acc  = dp.train_perceptron(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 8. Get creative ##\n",
    "\n",
    "This part is mandatory for 7650 and optional for 4650.\n",
    "\n",
    "**Deliverable 8 ** K-means clustering is one way to find similarity. This part is opened-ended and requires you to come up with creative solutions of using distributed vectors to improve the results. In addition to implementing your idea, provide explanation of what you did, and why you thought it should work.\n",
    "\n",
    "The top 3 selected ideas (based mostly on performance, but also on creativity) will get an extra point towards their final score.(5 points)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
